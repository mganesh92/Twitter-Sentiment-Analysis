sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
test <- ldply(mom,function(t) t$toDataFrame() )
terror_5
terror_5[1:10,]$word
test <- ldply(terror_5[1:10,]$word,function(t) t$toDataFrame() )
terror_5$word
test <- ldply(terror_5$word,function(t) t$toDataFrame() )
terror_5$word[1]
test <- ldply(mom,function(t) t$toDataFrame() )
test <- ldply(mom,function(t) t$toDataFrame() )
mom
test <- ldply(pos.words,function(t) t$toDataFrame() )
test <- ldply(terror,function(t) t$toDataFrame() )
test
test <- ldply(terror_finalCorpus,function(t) t$toDataFrame() )
test <- ldply(terro_text_corpus_nopunc_lc_noStopWrds_removeURL,function(t) t$toDataFrame() )
test <- ldply(terro_text_corpus,function(t) t$toDataFrame() )
test <- ldply(terro_text,function(t) t$toDataFrame() )
terror$words[1:10]
terror$words[1:10,]
terror_5[1:10,]
terror_5$word[1:10,]
terror_5$word[1:10]
terror_5$word
a<-terror_5$word[1:10]
a
a[1]
a[2]
terror_5$word[1:10]
terror_5$word[1:10,]
terror_5[1:10,]$word
a
names(terror_5)
names(terror_5[1:10,]$word)
names(terror_4)
test <- ldply(names(terror_4),function(t) t$toDataFrame() )
test <- laply(names(terror_4), function(t) t$getText() )
test <- laply(terror, function(t) t$getText() )
test[1]
terror_finalCorpus[1]
terro_text_corpus_nopunc_lc_noStopWrds_removeURL[1]
terro_text_corpus_nopunc_lc_noStopWrds_removeURL$content
terro_text_corpus_nopunc_lc_noStopWrds_removeURL$content[1]
terror_finalCorpus$content[1]
terror[1]
test <- laply(terror_finalCorpus$content, function(t) t$getText() )
a<-terror_finalCorpus$content
a[1]
terror[1]
a
terror
test <- laply(a, function(t) t$getText() )
type(terror)
typeof(terror)
typeof(a)
a.list()
length(a)
length(terror)
nl <- vector(mode="list", length=length(a)-1)
n1
names(nl) <- a[-1]
nl <- lapply(nl, function(x) a[1])
n1[1]
n1
nl[1]
list(a)
b<-list(a)
terror[1]
b[1]
rm(b)
test <- ldply(a,function(t) t$toDataFrame() )
test <- ldply(terror,function(t) t$toDataFrame() )
test <- ldply(terror,function(t) t$toDataFrame() )
test[1]
test[1]
test$text[1]
test <- ldply(a,function(t) t$toDataFrame() )
b<-as.list(a)
b[1]
terror[1]
test <- ldply(b,function(t) t$toDataFrame() )
typeof(terror)
typeof(b)
test <- laply(b, function(t) t$getText() )
terror[1]
b[1]
test <- ldply(terror,function(t) t$toDataFrame() )
test <- ldply(terror$text,function(t) t$toDataFrame() )
test
terror$text
terror$text[1]
b[1]
result <- score.sentiment(b,pos.words,neg.words)
a<-terror_finalCorpus$content
result <- score.sentiment(b,pos.words,neg.words)
result <- score.sentiment(b,pos.words,neg.words)
result <- score.sentiment(b,pos.words,neg.words)
test <- ldply(terror$text,function(t) t$toDataFrame() )
result <- score.sentiment(test$text,pos.words,neg.words)
summary(result$score)
hist(result$score,col ="yellow", main ="Score of tweets",
+      ylab = " Count of tweets")
hist(result$score,col ="yellow", main ="Score of tweets",+ ylab = " Count of tweets")
hist(result$score,col = "yellow",main = "score", + ylab = "count of tweets")
hist(result$score)
result$score
test$text
test
test <- ldply(terror$text,function(t) t$toDataFrame() )
tweets.text=laply(terror,function(t)t$getText())
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list
# or a vector as an "l" for us
# we want a simple array ("a") of scores back, so we use
# "l" + "a" + "ply" = "laply":
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
analysis = score.sentiment(tweets.text, pos.words, neg.words)
scores.sentiment = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.sentiment = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
scores.sentiment = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
tweets.text=laply(terror,function(t)t$getText())
analysis = score.sentiment(tweets.text, pos.words, neg.words)
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list
# or a vector as an “l” for us
# we want a simple array (“a”) of scores back, so we use
# “l” + “a” + “ply” = “laply”:
scores = laply(sentences, function(sentence, pos.words, neg.words) {
# clean up sentences with R’s regex-driven global substitute, gsub():
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
# and convert to lower case:
sentence = tolower(sentence)
# split into words. str_split is in the stringr package
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
tweets.text=laply(terror,function(t)t$getText())
analysis = score.sentiment(tweets.text, pos.words, neg.words)
table(analysis$score)
hist(analysis$score)
tweets.text=laply(b,function(t)t$getText())
terror
tweets.text=laply(terror,function(t)t$getText())
tweets.text[1]
a[1]
analysis = score.sentiment(a, pos.words, neg.words)
table(analysis$score)
hist(analysis$score)
dev.off()
hist(analysis$score)
hist(analysis$score)
summary(analysis$score)
qplot(analysis$score,xlab = "Score of tweets")
library(qplot)
install.packages("qplot")
count(analysis$score)
write.xlsx(analysis, "myResults.xlsx")
library(xlsx)
install.packages("xlsk")
install.packages('xlsx', type='source', repos='http://cran.rstudio.com')
library(xlsx)
library(xlsx)
install.packages("xlsx_0.4.2.tar.gz", type="source")
java -version
install.packages("rJava",,"http://rforge.net/",type="source")
library(xlsx)
install.packages('xlsx', type='source', repos='http://cran.rstudio.com')
library(xlsx)
DONE(rJava)
library(xlsx)
write.csv(analysis, "myResults.csv")
qplot(analysis$score,xlab = "Score of tweets")
hist(analysis$score)
terror <- searchTwitter("#oviaarmy",n=5000)
terro_text <- sapply(terror, function(x) x$getText())
terro_text_corpus <- Corpus(VectorSource(terro_text))
# clean the data
#1. remove punctuations
#2. convert all to lower case
#3. remove stopwords or remove the words which we may feel may be trivial
terro_text_corpus_nopunc <- tm_map(terro_text_corpus, removePunctuation)
terro_text_corpus_nopunc_lc <- tm_map(terro_text_corpus_nopunc, content_transformer(tolower))
terro_text_corpus_nopunc_lc_noStopWrds <- tm_map(terro_text_corpus_nopunc_lc, function(x)removeWords(x,stopwords()))
#remove URL from the comments
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
terro_text_corpus_nopunc_lc_noStopWrds_removeURL <- tm_map(terro_text_corpus_nopunc_lc_noStopWrds, content_transformer(removeURL))
#build a term document matrix. this would put the text in alphabetical order matrix
terror_finalCorpus<-terro_text_corpus_nopunc_lc_noStopWrds_removeURL
# create document matrix with frequency of words from all the tweets
terror_2 <- TermDocumentMatrix(terror_finalCorpus)
terror_3 <- as.matrix(terror_2)
terror_4 <- sort(rowSums(terror_3),decreasing=TRUE)
#convert words to data frames
terror_5 <- data.frame(word = names(terror_4),freq=terror_4)
head(terror_5,10)
barplot(terror_5[1:10,]$freq, las = 2, names.arg = terror_5[1:10,]$word,
col ="yellow", main ="Most frequent words",
ylab = "Word frequencies")
#I used dev.off if the margin error occurs
#adjust the frequency to show the most influence of the words...at fre=1000...only barcelona and terrorism shows up
wordcloud(terror_finalCorpus,min.freq=100,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
wordcloud(terror_finalCorpus,min.freq=1000,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
terror <- searchTwitter("#ASU",n=5000)
terro_text <- sapply(terror, function(x) x$getText())
terro_text_corpus <- Corpus(VectorSource(terro_text))
# clean the data
#1. remove punctuations
#2. convert all to lower case
#3. remove stopwords or remove the words which we may feel may be trivial
terro_text_corpus_nopunc <- tm_map(terro_text_corpus, removePunctuation)
terro_text_corpus_nopunc_lc <- tm_map(terro_text_corpus_nopunc, content_transformer(tolower))
terro_text_corpus_nopunc_lc_noStopWrds <- tm_map(terro_text_corpus_nopunc_lc, function(x)removeWords(x,stopwords()))
#remove URL from the comments
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
terro_text_corpus_nopunc_lc_noStopWrds_removeURL <- tm_map(terro_text_corpus_nopunc_lc_noStopWrds, content_transformer(removeURL))
#build a term document matrix. this would put the text in alphabetical order matrix
terror_finalCorpus<-terro_text_corpus_nopunc_lc_noStopWrds_removeURL
# create document matrix with frequency of words from all the tweets
terror_2 <- TermDocumentMatrix(terror_finalCorpus)
terror_3 <- as.matrix(terror_2)
terror_4 <- sort(rowSums(terror_3),decreasing=TRUE)
#convert words to data frames
terror_5 <- data.frame(word = names(terror_4),freq=terror_4)
head(terror_5,10)
barplot(terror_5[1:10,]$freq, las = 2, names.arg = terror_5[1:10,]$word,
col ="yellow", main ="Most frequent words",
ylab = "Word frequencies")
#I used dev.off if the margin error occurs
#adjust the frequency to show the most influence of the words...at fre=1000...only barcelona and terrorism shows up
wordcloud(terror_finalCorpus,min.freq=1000,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
wordcloud(terror_finalCorpus,min.freq=10,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
wordcloud(terror_finalCorpus,min.freq=100,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
terror <- searchTwitter("#vydehihospital",n=5000)
terro_text <- sapply(terror, function(x) x$getText())
terro_text_corpus <- Corpus(VectorSource(terro_text))
# clean the data
#1. remove punctuations
#2. convert all to lower case
#3. remove stopwords or remove the words which we may feel may be trivial
terro_text_corpus_nopunc <- tm_map(terro_text_corpus, removePunctuation)
terro_text_corpus_nopunc_lc <- tm_map(terro_text_corpus_nopunc, content_transformer(tolower))
terro_text_corpus_nopunc_lc_noStopWrds <- tm_map(terro_text_corpus_nopunc_lc, function(x)removeWords(x,stopwords()))
#remove URL from the comments
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
terro_text_corpus_nopunc_lc_noStopWrds_removeURL <- tm_map(terro_text_corpus_nopunc_lc_noStopWrds, content_transformer(removeURL))
#build a term document matrix. this would put the text in alphabetical order matrix
terror_finalCorpus<-terro_text_corpus_nopunc_lc_noStopWrds_removeURL
# create document matrix with frequency of words from all the tweets
terror_2 <- TermDocumentMatrix(terror_finalCorpus)
terror_3 <- as.matrix(terror_2)
terror_4 <- sort(rowSums(terror_3),decreasing=TRUE)
#convert words to data frames
terror_5 <- data.frame(word = names(terror_4),freq=terror_4)
head(terror_5,10)
barplot(terror_5[1:10,]$freq, las = 2, names.arg = terror_5[1:10,]$word,
col ="yellow", main ="Most frequent words",
ylab = "Word frequencies")
#I used dev.off if the margin error occurs
#adjust the frequency to show the most influence of the words...at fre=1000...only barcelona and terrorism shows up
wordcloud(terror_finalCorpus,min.freq=100,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
wordcloud(terror_finalCorpus,min.freq=10,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
head(terror_5,10)
terror[1]
terror <- searchTwitter("#vydehihospital",n=5000)
terror
terror <- searchTwitter("#VydehiHospital",n=5000)
terror
terror <- searchTwitter("#VydehiHospital",n=10)
terror <- searchTwitter("#Happiness",n=10)
terror <- searchTwitter("#Happiness",n=1000)
terro_text <- sapply(terror, function(x) x$getText())
terro_text_corpus <- Corpus(VectorSource(terro_text))
# clean the data
#1. remove punctuations
#2. convert all to lower case
#3. remove stopwords or remove the words which we may feel may be trivial
terro_text_corpus_nopunc <- tm_map(terro_text_corpus, removePunctuation)
terro_text_corpus_nopunc_lc <- tm_map(terro_text_corpus_nopunc, content_transformer(tolower))
terro_text_corpus_nopunc_lc_noStopWrds <- tm_map(terro_text_corpus_nopunc_lc, function(x)removeWords(x,stopwords()))
#remove URL from the comments
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
terro_text_corpus_nopunc_lc_noStopWrds_removeURL <- tm_map(terro_text_corpus_nopunc_lc_noStopWrds, content_transformer(removeURL))
#build a term document matrix. this would put the text in alphabetical order matrix
terror_finalCorpus<-terro_text_corpus_nopunc_lc_noStopWrds_removeURL
# create document matrix with frequency of words from all the tweets
terror_2 <- TermDocumentMatrix(terror_finalCorpus)
terror_3 <- as.matrix(terror_2)
terror_4 <- sort(rowSums(terror_3),decreasing=TRUE)
#convert words to data frames
terror_5 <- data.frame(word = names(terror_4),freq=terror_4)
head(terror_5,10)
barplot(terror_5[1:10,]$freq, las = 2, names.arg = terror_5[1:10,]$word,
col ="yellow", main ="Most frequent words",
ylab = "Word frequencies")
#I used dev.off if the margin error occurs
#adjust the frequency to show the most influence of the words...at fre=1000...only barcelona and terrorism shows up
wordcloud(terror_finalCorpus,min.freq=10,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
wordcloud(terror_finalCorpus,min.freq=1000,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
wordcloud(terror_finalCorpus,min.freq=500,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
terror <- searchTwitter("#WeightLoss",n=1000)
terro_text <- sapply(terror, function(x) x$getText())
terro_text_corpus <- Corpus(VectorSource(terro_text))
# clean the data
#1. remove punctuations
#2. convert all to lower case
#3. remove stopwords or remove the words which we may feel may be trivial
terro_text_corpus_nopunc <- tm_map(terro_text_corpus, removePunctuation)
terro_text_corpus_nopunc_lc <- tm_map(terro_text_corpus_nopunc, content_transformer(tolower))
terro_text_corpus_nopunc_lc_noStopWrds <- tm_map(terro_text_corpus_nopunc_lc, function(x)removeWords(x,stopwords()))
#remove URL from the comments
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
terro_text_corpus_nopunc_lc_noStopWrds_removeURL <- tm_map(terro_text_corpus_nopunc_lc_noStopWrds, content_transformer(removeURL))
#build a term document matrix. this would put the text in alphabetical order matrix
terror_finalCorpus<-terro_text_corpus_nopunc_lc_noStopWrds_removeURL
# create document matrix with frequency of words from all the tweets
terror_2 <- TermDocumentMatrix(terror_finalCorpus)
terror_3 <- as.matrix(terror_2)
terror_4 <- sort(rowSums(terror_3),decreasing=TRUE)
#convert words to data frames
terror_5 <- data.frame(word = names(terror_4),freq=terror_4)
head(terror_5,10)
barplot(terror_5[1:10,]$freq, las = 2, names.arg = terror_5[1:10,]$word,
col ="yellow", main ="Most frequent words",
ylab = "Word frequencies")
#I used dev.off if the margin error occurs
#adjust the frequency to show the most influence of the words...at fre=1000...only barcelona and terrorism shows up
wordcloud(terror_finalCorpus,min.freq=500,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
wordcloud(terror_finalCorpus,min.freq=100,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
terror <- searchTwitter("#Trump",n=1000)
terro_text <- sapply(terror, function(x) x$getText())
terro_text_corpus <- Corpus(VectorSource(terro_text))
# clean the data
#1. remove punctuations
#2. convert all to lower case
#3. remove stopwords or remove the words which we may feel may be trivial
terro_text_corpus_nopunc <- tm_map(terro_text_corpus, removePunctuation)
terro_text_corpus_nopunc_lc <- tm_map(terro_text_corpus_nopunc, content_transformer(tolower))
terro_text_corpus_nopunc_lc_noStopWrds <- tm_map(terro_text_corpus_nopunc_lc, function(x)removeWords(x,stopwords()))
#remove URL from the comments
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
terro_text_corpus_nopunc_lc_noStopWrds_removeURL <- tm_map(terro_text_corpus_nopunc_lc_noStopWrds, content_transformer(removeURL))
#build a term document matrix. this would put the text in alphabetical order matrix
terror_finalCorpus<-terro_text_corpus_nopunc_lc_noStopWrds_removeURL
# create document matrix with frequency of words from all the tweets
terror_2 <- TermDocumentMatrix(terror_finalCorpus)
terror_3 <- as.matrix(terror_2)
terror_4 <- sort(rowSums(terror_3),decreasing=TRUE)
#convert words to data frames
terror_5 <- data.frame(word = names(terror_4),freq=terror_4)
head(terror_5,10)
barplot(terror_5[1:10,]$freq, las = 2, names.arg = terror_5[1:10,]$word,
col ="yellow", main ="Most frequent words",
ylab = "Word frequencies")
#I used dev.off if the margin error occurs
#adjust the frequency to show the most influence of the words...at fre=1000...only barcelona and terrorism shows up
wordcloud(terror_finalCorpus,min.freq=100,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
wordcloud(terror_finalCorpus,min.freq=1000,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
wordcloud(terror_finalCorpus,min.freq=10000,max.words=80,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
wordcloud(terror_finalCorpus,min.freq=10000,max.words=10,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
wordcloud(terror_finalCorpus,min.freq=1000000,max.words=10,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
wordcloud(terror_finalCorpus,min.freq=1000000,max.words=1,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
wordcloud(terror_finalCorpus,min.freq=1000000,max.words=2,scale=c(2.2,1), colors=brewer.pal(8, "Dark2"),random.color=T, random.order=F)
